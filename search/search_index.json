{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":"<p>Welcome to Generative Information Retrieval Reading Notes.</p> <p>This site provides a curated and continuously updated collection of papers related to GenIR, covering three major research directions:</p> <ul> <li>Generative Document Retrieval</li> <li>Generative Recommendation</li> <li>Generative Multimodal Retrieval &amp; Recommendation</li> </ul> <p>For each paper, we provide a concise summary, key ideas, relevant links, and brief categorization to help readers quickly understand the core contributions and the broader landscape of generative retrieval research.</p> <p>If you prefer a more systematic and structured overview of the field, you may refer to our [tutorial]. If you want to quickly get started without reading lengthy materials, visit our Get Started page, which summarizes the essential background knowledge, resources, and datasets you will need.</p> <p>We will continue to expand and maintain this repository as the field evolves. We hope this resource helps researchers and students navigate the rapidly growing literature on GenIR.</p>"},{"location":"recent/","title":"\ud83d\udd52 Recent Updates","text":"<p>Below is a chronological list of all recently added or updated papers across Document Retrieval, Recommendation, and Multimodal Retrieval.</p> <p> AAAAAAAAAA (Tay et al. 2022)        [PDF]       [Code]      </p> <p>     Tags:    title MS MARCO Doc </p> <p>BBBBBBBBB (Tay et al. 2022) [PDF] [Code] </p> <p> Tags:    title NQ </p> <p>CCCCCCCC (Tay et al. 2022)  [PDF] [Code] </p> <p> Tags:    title MS MARCO Doc </p>"},{"location":"recent/#this-week","title":"\ud83d\udd34 This Week","text":""},{"location":"recent/#last-week","title":"\ud83d\udfe2 Last Week","text":""},{"location":"recent/#earlier","title":"\ud83d\udfe1 Earlier","text":""},{"location":"resources/","title":"Resources for Getting Started with Generative IR","text":"<p>This page provides a curated set of entry-level resources for students and researchers who want to quickly understand and start working on Generative Information Retrieval (GenIR), Generative Recommendation (GenRec), and Multimodal Generative Retrieval.</p> <p>It includes tutorials, starter code, datasets, beginner-friendly papers, and tools.</p>"},{"location":"resources/#1-what-is-generative-ir","title":"\ud83d\udd39 1. What is Generative IR?","text":"<p>Generative IR treats retrieval as a sequence generation problem. Instead of scoring documents (dense/sparse retrieval), a model directly generates document identifiers (DocIDs) conditioned on a query.</p> <p>This paradigm enables: - flexible reasoning - multimodal integration - unifying retrieval with generation - direct indexing via generative models</p>"},{"location":"resources/#2-tutorials-surveys","title":"\ud83d\udd39 2. Tutorials &amp; Surveys","text":"<p>Our tutorial (recommended starting point): \ud83d\udc49 GenIR Tutorial</p> <p>Other helpful materials: - Generative Search and Retrieval (2024) \u2014 overview slides - The rise of generative retrieval models (2023) \u2014 survey - SIGIR/ECIR tutorial slides (if any available public) </p> <p>You may also find NeurIPS and ICLR spotlight videos helpful for high-level intuition.</p>"},{"location":"resources/#3-starter-code-repositories","title":"\ud83d\udd39 3. Starter Code Repositories","text":"<p>Begin with small, runnable implementations:</p> <ul> <li> <p>Minimal T5-based Generative Retriever   https://github.com/example/t5-simple-genir</p> </li> <li> <p>GR2-style constrained decoding demo   https://github.com/example/gr2-decoding-demo</p> </li> <li> <p>Prefix-tree decoding example   https://github.com/example/prefix-tree-search</p> </li> <li> <p>BM25-labeled GR Training   A simple script: generate docIDs from BM25 labels and train a seq2seq model.</p> </li> </ul> <p>Recommended starting exercise: \ud83d\udc49 Train a small T5 model to generate document IDs on a tiny dataset.</p>"},{"location":"resources/#4-datasets-for-experiments","title":"\ud83d\udd39 4. Datasets for Experiments","text":""},{"location":"resources/#text-retrieval","title":"Text Retrieval","text":"<ul> <li>MS MARCO (Passage / Document) </li> <li>BEIR (benchmark suite)  </li> <li>Natural Questions (NQ) </li> <li>MIRACL</li> </ul>"},{"location":"resources/#multimodal-retrieval","title":"Multimodal Retrieval","text":"<ul> <li>MS-COCO </li> <li>Flickr30K </li> <li>BookCover30K </li> <li>WhatsThatBook </li> </ul>"},{"location":"resources/#for-teaching-prototyping","title":"For Teaching / Prototyping","text":"<ul> <li>Small Wikipedia slices  </li> <li>QA subsets  </li> <li>Your own prepared toy datasets</li> </ul>"},{"location":"resources/#5-beginner-start-reading-recommendations","title":"\ud83d\udd39 5. Beginner start reading recommendations","text":"<p>Recommended Entry Path (Beginner \u219d Intermediate \u219d Modern):</p> <ol> <li>GENRE (2020) \u2014 first works on generative retrieval  </li> <li>T5 for Retrieval (2021) \u2014 simple baseline  </li> <li>ASI (ICLR 2023) \u2014 introduces hierarchical DocIDs  </li> <li>GR2 (ICLR 2023) \u2014 constrained decoding  </li> <li>ReasonGR (NeurIPS 2024) \u2014 reasoning-based decoding  </li> <li>DCI (SIGIR 2025) \u2014 discriminative indexing  </li> <li>NOVA (AAAI 2026) \u2014 unified-to-specialized architecture</li> </ol> <p>Reading these gives a complete understanding of the field\u2019s evolution.</p>"},{"location":"resources/#6-tools-practical-components","title":"\ud83d\udd39 6. Tools &amp; Practical Components","text":"<p>Helpful elements commonly used in GR systems:</p> <ul> <li>Prefix tree builder</li> <li>ID quantization / clustering</li> <li>DocID templates (hierarchical IDs)</li> <li>Synthetic query generators</li> <li>Constrained beam search tools</li> <li>Visual-text encoders (for multimodal work)</li> </ul> <p>We recommend building a minimal pipeline:</p> <p>query \u2192 encoder \u2192 decoder \u2192 docID \u2192 matching \u2192 evaluation</p> <p>to get hands-on experience.</p>"},{"location":"resources/#7-extra-joining-the-community","title":"\ud83d\udd39 7. Extra: Joining the Community","text":"<ul> <li>SIGIR paper reading groups  </li> <li>Open-source GR communities  </li> <li>Relevant GitHub organizations and curated lists  </li> <li>Discord/Slack groups (if applicable)</li> </ul> <p>Feel free to suggest new resources \u2014 this page is intended to grow with the community.</p>"},{"location":"document_retrieval/papers/","title":"Papers: Document Retrieval","text":"<p>This page organizes Generative Document Retrieval papers by their core innovation. To keep the page compact, each paper is listed using lightweight formatting (bold titles instead of section headers).</p>"},{"location":"document_retrieval/papers/#a-id-space-indexing-innovations","title":"\ud83d\udd37 A. ID Space &amp; Indexing Innovations","text":"<p> AAAAAAAAAA (Tay et al. 2022)        [PDF]       [Code]      </p> <p>     Tags:    title MS MARCO Doc </p> <p>BBBBBBBBB (Tay et al. 2022) [PDF] [Code] </p> <p> Tags:    title NQ </p> <p>CCCCCCCC (Tay et al. 2022)  [PDF] [Code] </p> <p> Tags:    title MS MARCO Doc </p>"},{"location":"document_retrieval/papers/#b-training-paradigms-learning-signals","title":"\ud83d\udd37 B. Training Paradigms &amp; Learning Signals","text":""},{"location":"document_retrieval/papers/#c-model-architecture-innovations","title":"\ud83d\udd37 C. Model Architecture Innovations","text":""},{"location":"document_retrieval/papers/#d-decoding-inference-innovations","title":"\ud83d\udd37 D. Decoding &amp; Inference Innovations","text":""},{"location":"document_retrieval/roadmap/","title":"Identifier Design Roadmap","text":"<p>This roadmap provides an overview of predefined document identifier (docid) design strategies in GenIR.  Docid design is typically organized around two major categories: Single-docid (using one docid to represent the doc) and Multi-docid (using multiple docids to represent the doc).  A Single docid can be constructed using either number-based docids or word-based docids, each representing different encoding principles.  These individual docid types can also be combined or extended to form Multi-docid structures, which aim to capture richer or more diverse document representations.</p> <p>The diagram below summarizes the relationships among these design families and lists representative approaches for each category.</p> <pre><code>flowchart LR\n\n    %% Main categories\n    A[\"Single-docid\"]\n\n    A --&gt; A1[\"Number-based docid\"]\n    A --&gt; A2[\"Word-based docid\"]\n\n    %% Number-based docids breakdown\n    A1 --&gt; A1a[\"Unstructured atomic integers &lt;br&gt;  &lt;a href='https://arxiv.org/pdf/2202.06991' target='_blank'&gt;(Tay et al. 2022)&lt;/a&gt;\"]\n    A1 --&gt; A1b[\"Naively structured strings &lt;br&gt;  &lt;a href='https://arxiv.org/pdf/2202.06991' target='_blank'&gt;(Tay et al. 2022)&lt;/a&gt;\"]\n    A1 --&gt; A1c[\"Semantically structured strings &lt;br&gt;  &lt;a href='https://arxiv.org/pdf/2202.06991' target='_blank'&gt;(Tay et al. 2022)&lt;/a&gt;\"]\n    A1 --&gt; A1d[\"Product quantization strings &lt;br&gt; &lt;a href='https://arxiv.org/pdf/2208.09257' target='_blank'&gt;(Zhou et al. 2022)&lt;/a&gt;\"]\n\n    %% Word-based docids breakdown\n    A2 --&gt; A2a[\"Titles &lt;br&gt; &lt;a href='https://arxiv.org/pdf/2010.00904' target='_blank'&gt;(De Cao et al. 2021)&lt;/a&gt;\"]\n    A2 --&gt; A2b[\"URLs &lt;br&gt;  &lt;a href='https://arxiv.org/pdf/2208.09257' target='_blank'&gt;(Zhou et al. 2022)&lt;/a&gt;\"]\n    A2 --&gt; A2c[\"Pseudo queries &lt;br&gt; &lt;a href='https://arxiv.org/pdf/2305.15115' target='_blank'&gt;(Tang et al. 2023a)&lt;/a&gt;\"]\n    A2 --&gt; A2d[\"N-grams &lt;br&gt;  &lt;a href='https://arxiv.org/pdf/2204.10628' target='_blank'&gt;(Bevilacqua et al. 2022)&lt;/a&gt;\"]\n\n    %% Single \u2192 Multiple\n    B[\"Multi-docid\"]\n    A --&gt; B</code></pre>"},{"location":"document_retrieval/roadmap/#todo","title":"TODO","text":"<ul> <li>[ ] Add training roadmap</li> </ul>"}]}