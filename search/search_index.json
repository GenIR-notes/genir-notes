{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":"<p>Welcome to Generative Information Retrieval Reading Notes.</p> <p>This site provides a curated and continuously updated collection of papers related to GenIR, covering three major research directions:</p> <ul> <li>Generative Document Retrieval</li> <li>Generative Recommendation</li> <li>Generative Multimodal Retrieval &amp; Recommendation</li> </ul> <p>For each paper, we provide a concise summary, key ideas, relevant links, and brief categorization to help readers quickly understand the core contributions and the broader landscape of generative retrieval research.</p> <p>If you prefer a more systematic and structured overview of the field, you may refer to our [tutorial]. If you want to quickly get started without reading lengthy materials, visit our Get Started page, which summarizes the essential background knowledge, resources, and datasets you will need.</p> <p>We will continue to expand and maintain this repository as the field evolves. We hope this resource helps researchers and students navigate the rapidly growing literature on GenIR.</p>"},{"location":"resources/","title":"Resources for Getting Started with Generative IR","text":"<p>This page provides a curated set of entry-level resources for students and researchers who want to quickly understand and start working on Generative Information Retrieval (GenIR), Generative Recommendation (GenRec), and Multimodal Generative Retrieval.</p> <p>It includes tutorials, starter code, datasets, beginner-friendly papers, and tools.</p>"},{"location":"resources/#1-what-is-generative-ir","title":"\ud83d\udd39 1. What is Generative IR?","text":"<p>Generative IR treats retrieval as a sequence generation problem. Instead of scoring documents (dense/sparse retrieval), a model directly generates document identifiers (DocIDs) conditioned on a query.</p> <p>This paradigm enables: - flexible reasoning - multimodal integration - unifying retrieval with generation - direct indexing via generative models</p>"},{"location":"resources/#2-tutorials-surveys","title":"\ud83d\udd39 2. Tutorials &amp; Surveys","text":"<p>Our tutorial (recommended starting point): \ud83d\udc49 GenIR Tutorial</p> <p>Other helpful materials: - Generative Search and Retrieval (2024) \u2014 overview slides - The rise of generative retrieval models (2023) \u2014 survey - SIGIR/ECIR tutorial slides (if any available public) </p> <p>You may also find NeurIPS and ICLR spotlight videos helpful for high-level intuition.</p>"},{"location":"resources/#3-starter-code-repositories","title":"\ud83d\udd39 3. Starter Code Repositories","text":"<p>Begin with small, runnable implementations:</p> <ul> <li> <p>Minimal T5-based Generative Retriever   https://github.com/example/t5-simple-genir</p> </li> <li> <p>GR2-style constrained decoding demo   https://github.com/example/gr2-decoding-demo</p> </li> <li> <p>Prefix-tree decoding example   https://github.com/example/prefix-tree-search</p> </li> <li> <p>BM25-labeled GR Training   A simple script: generate docIDs from BM25 labels and train a seq2seq model.</p> </li> </ul> <p>Recommended starting exercise: \ud83d\udc49 Train a small T5 model to generate document IDs on a tiny dataset.</p>"},{"location":"resources/#4-datasets-for-experiments","title":"\ud83d\udd39 4. Datasets for Experiments","text":""},{"location":"resources/#text-retrieval","title":"Text Retrieval","text":"<ul> <li>MS MARCO (Passage / Document) </li> <li>BEIR (benchmark suite)  </li> <li>Natural Questions (NQ) </li> <li>MIRACL</li> </ul>"},{"location":"resources/#multimodal-retrieval","title":"Multimodal Retrieval","text":"<ul> <li>MS-COCO </li> <li>Flickr30K </li> <li>BookCover30K </li> <li>WhatsThatBook </li> </ul>"},{"location":"resources/#for-teaching-prototyping","title":"For Teaching / Prototyping","text":"<ul> <li>Small Wikipedia slices  </li> <li>QA subsets  </li> <li>Your own prepared toy datasets</li> </ul>"},{"location":"resources/#5-beginner-start-reading-recommendations","title":"\ud83d\udd39 5. Beginner start reading recommendations","text":"<p>Recommended Entry Path (Beginner \u219d Intermediate \u219d Modern):</p> <ol> <li>GENRE (2020) \u2014 first works on generative retrieval  </li> <li>T5 for Retrieval (2021) \u2014 simple baseline  </li> <li>ASI (ICLR 2023) \u2014 introduces hierarchical DocIDs  </li> <li>GR2 (ICLR 2023) \u2014 constrained decoding  </li> <li>ReasonGR (NeurIPS 2024) \u2014 reasoning-based decoding  </li> <li>DCI (SIGIR 2025) \u2014 discriminative indexing  </li> <li>NOVA (AAAI 2026) \u2014 unified-to-specialized architecture</li> </ol> <p>Reading these gives a complete understanding of the field\u2019s evolution.</p>"},{"location":"resources/#6-tools-practical-components","title":"\ud83d\udd39 6. Tools &amp; Practical Components","text":"<p>Helpful elements commonly used in GR systems:</p> <ul> <li>Prefix tree builder</li> <li>ID quantization / clustering</li> <li>DocID templates (hierarchical IDs)</li> <li>Synthetic query generators</li> <li>Constrained beam search tools</li> <li>Visual-text encoders (for multimodal work)</li> </ul> <p>We recommend building a minimal pipeline:</p> <p>query \u2192 encoder \u2192 decoder \u2192 docID \u2192 matching \u2192 evaluation</p> <p>to get hands-on experience.</p>"},{"location":"resources/#7-extra-joining-the-community","title":"\ud83d\udd39 7. Extra: Joining the Community","text":"<ul> <li>SIGIR paper reading groups  </li> <li>Open-source GR communities  </li> <li>Relevant GitHub organizations and curated lists  </li> <li>Discord/Slack groups (if applicable)</li> </ul> <p>Feel free to suggest new resources \u2014 this page is intended to grow with the community.</p>"},{"location":"document_retrieval/papers/","title":"Papers: Document Retrieval","text":"<p>This page organizes Generative Document Retrieval papers by their core innovation. To keep the page compact, each paper is listed using lightweight formatting (bold titles instead of section headers).</p>"},{"location":"document_retrieval/papers/#a-id-space-indexing-innovations","title":"\ud83d\udd37 A. ID Space &amp; Indexing Innovations","text":"<p>ASI \u2013 Autoregressive Search Index (ICLR 2023) \ud83d\udd17 paper Hierarchical document IDs with multi-level tree structure for controlled ID generation. Tags: <code>hierarchical-id</code>, <code>indexing</code></p> <p>SEATER (2023) \ud83d\udd17 paper Structured tree IDs to reduce ambiguity by decomposing GR errors. Tags: <code>structured-id</code>, <code>error-analysis</code></p> <p>DCI \u2013 Discriminative Contrastive Indexing (SIGIR 2025) \ud83d\udd17 paper (placeholder) Uses groupwise contrastive learning to produce discriminative document IDs. Tags: <code>contrastive</code>, <code>id-learning</code></p> <p>Corpus-Aligned ID Initialization (2024) \ud83d\udd17 paper Initializes docIDs using synthetic queries aligned with corpus semantics. Tags: <code>id-initialization</code>, <code>query-synthesis</code></p>"},{"location":"document_retrieval/papers/#b-training-paradigms-learning-signals","title":"\ud83d\udd37 B. Training Paradigms &amp; Learning Signals","text":"<p>BM25-Labeled GR Training (2023) \ud83d\udd17 paper Trains GR using BM25 pseudo relevance labels, enabling training without human annotations. Tags: <code>pseudo-label</code>, <code>weak-supervision</code></p> <p>Synthetic Query Learning (2024) \ud83d\udd17 paper (placeholder) Uses LLM-generated synthetic queries to improve GR training and corpus coverage. Tags: <code>synthetic-data</code>, <code>pretraining</code></p> <p>Contrastive Hybrid Training (2025) \ud83d\udd17 paper (placeholder) Combines generative decoding loss with contrastive objectives for discriminative docIDs. Tags: <code>hybrid-training</code>, <code>contrastive</code></p>"},{"location":"document_retrieval/papers/#c-model-architecture-innovations","title":"\ud83d\udd37 C. Model Architecture Innovations","text":"<p>GENRE (2020) \ud83d\udd17 paper \u00b7 code Early seq2seq generative retrieval using entity generation. Tags: <code>seq2seq</code>, <code>entity-retrieval</code></p> <p>T5 for Retrieval (2021) \ud83d\udd17 paper Demonstrates encoder\u2013decoder LMs can generate document IDs directly. Tags: <code>t5</code>, <code>encoder-decoder</code></p> <p>NOVA \u2013 Unified-to-Specialized Framework (AAAI 2026) \ud83d\udd17 paper (placeholder) Multi-view encoder with specialized decoders for cross-modal and text-only retrieval. Tags: <code>multi-view</code>, <code>decoder</code>, <code>architecture</code></p>"},{"location":"document_retrieval/papers/#d-decoding-inference-innovations","title":"\ud83d\udd37 D. Decoding &amp; Inference Innovations","text":"<p>GR2 \u2013 Guided Decoding for GR (ICLR 2023) \ud83d\udd17 paper Adds constrained / guided decoding for more accurate ID generation. Tags: <code>guided-decoding</code>, <code>constraint</code></p> <p>ReasonGR \u2013 Multi-Step Reasoning (NeurIPS 2024) \ud83d\udd17 paper Models multi-hop reasoning during decoding to improve retrieval on complex queries. Tags: <code>reasoning</code>, <code>multihop</code></p> <p>CGBS Decoding (2025) \ud83d\udd17 paper (placeholder) Cover-aware and constrained decoding for multimodal ID generation. Tags: <code>cover-aware</code>, <code>multimodal</code>, <code>constraint</code></p>"},{"location":"document_retrieval/roadmap/","title":"Roadmap","text":"<pre><code>flowchart LR\n\n    %% Main categories\n    A[\"A single docid\"]\n    B[\"Multiple docids\"]\n\n    A --&gt; A1[\"Number-based docids\"]\n    A --&gt; A2[\"Word-based docids\"]\n\n    %% Number-based docids breakdown\n    A1 --&gt; A1a[\"Unstructured atomic integers&lt;br&gt;(Tay et al. 2022)\"]\n    A1 --&gt; A1b[\"Naively structured strings&lt;br&gt;(Tay et al. 2022)\"]\n    A1 --&gt; A1c[\"Semantically structured strings&lt;br&gt;(Tay et al. 2022)\"]\n    A1 --&gt; A1d[\"Product quantization strings&lt;br&gt;(Zhou et al. 2022)\"]\n\n    %% Word-based docids breakdown\n    A2 --&gt; A2a[\"Titles&lt;br&gt;(De Cao et al. 2021)\"]\n    A2 --&gt; A2b[\"URLs&lt;br&gt;(Zhou et al. 2022)\"]\n    A2 --&gt; A2c[\"Pseudo queries&lt;br&gt;(Tang et al. 2023a)\"]\n    A2 --&gt; A2d[\"Important terms&lt;br&gt;(Zhang et al. 2023)\"]\n\n    %% Single \u2192 Multiple\n    A --&gt; B</code></pre>"}]}